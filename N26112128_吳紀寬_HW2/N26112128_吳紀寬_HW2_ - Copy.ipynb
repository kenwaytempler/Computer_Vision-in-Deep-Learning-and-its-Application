{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zX5qnIteT4Qn"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers,models\n",
        "import pandas as pd\n",
        "import os\n",
        "import cv2"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1: Data Preprocessing"
      ],
      "metadata": {
        "id": "QQU3oHtHntoJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Data"
      ],
      "metadata": {
        "id": "FiPqvCztUD4m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        },
        "id": "mCNJfnSI0iWE",
        "outputId": "0603f22e-a6a3-4c47-95f5-239f1125cbd9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-d5df0069828e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    130\u001b[0m   )\n\u001b[1;32m    131\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 設定資料路徑\n",
        "# training data path\n",
        "trcloudy='/content/drive/MyDrive/weather_image/train/cloudy/'\n",
        "trrain='/content/drive/MyDrive/weather_image/train/rain/'\n",
        "trshine='/content/drive/MyDrive/weather_image/train/shine/'\n",
        "trsunrise='/content/drive/MyDrive/weather_image/train/sunrise/'\n",
        "# testing data path\n",
        "testpath='/content/drive/MyDrive/weather_image/test/'"
      ],
      "metadata": {
        "id": "S67siOHUPtFs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"for i in range(len(os.listdir(trcloudy))):\n",
        "  f=os.listdir(trcloudy)[i]\n",
        "  img=cv2.imread(trcloudy+f)\n",
        "  img=data_augmentation(img)\n",
        "  img = np.float32(img)\n",
        "  cv2.imwrite(trcloudy + str(i) + 'augmentation.jpg', img)\n",
        "for i in range(len(os.listdir(trrain))):\n",
        "  f=os.listdir(trrain)[i]\n",
        "  img=cv2.imread(trrain+f)\n",
        "  img=data_augmentation(img)\n",
        "  img = np.float32(img)\n",
        "  cv2.imwrite(trrain + str(i) + 'augmentation.jpg', img)\n",
        "for i in range(len(os.listdir(trshine))):\n",
        "  f=os.listdir(trshine)[i]\n",
        "  img=cv2.imread(trshine+f)\n",
        "  img=data_augmentation(img)\n",
        "  img = np.float32(img)\n",
        "  cv2.imwrite(trshine + str(i) + 'augmentation.jpg', img)\n",
        "for i in range(len(os.listdir(trsunrise))):\n",
        "  f=os.listdir(trsunrise)[i]\n",
        "  img=cv2.imread(trsunrise+f)\n",
        "  img=data_augmentation(img)\n",
        "  img = np.float32(img)\n",
        "  cv2.imwrite(trsunrise + str(i) + 'augmentation.jpg', img)\"\"\""
      ],
      "metadata": {
        "id": "IpwRhsaq1k3u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 顯示資料筆數\n",
        "print('train cloudy length: ',len(os.listdir(trcloudy)))\n",
        "print('train rain length: ',len(os.listdir(trrain)))\n",
        "print('train shine length: ',len(os.listdir(trshine)))\n",
        "print('train sunrise length: ',len(os.listdir(trsunrise)))\n",
        "print('\\n')\n",
        "print('test data length: ',len(os.listdir(testpath)))"
      ],
      "metadata": {
        "id": "IMxQ690vZn6F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### convert image data to numpy"
      ],
      "metadata": {
        "id": "BqOzw5g4l-1B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# convert the train data to numpy\n",
        "tr_label=list()\n",
        "tr_data=np.empty(shape=(2096,100,100,3))\n",
        "x=0\n",
        "size = (100, 100)\n",
        "for i in range(len(os.listdir(trcloudy))):\n",
        "  f=os.listdir(trcloudy)[i]\n",
        "  img=cv2.imread(trcloudy+f)\n",
        "  img=cv2.resize(img,size)\n",
        "  img=img[:,:,::-1]/255\n",
        "  tr_data[x]=img\n",
        "  tr_label.append(0)\n",
        "  x+=1\n",
        "for i in range(len(os.listdir(trrain))):\n",
        "  f=os.listdir(trrain)[i]\n",
        "  img=cv2.imread(trrain+f)\n",
        "  img=cv2.resize(img,size)\n",
        "  img=img[:,:,::-1]/255\n",
        "  tr_data[x]=img\n",
        "  tr_label.append(1)\n",
        "  x+=1\n",
        "for i in range(len(os.listdir(trshine))):\n",
        "  f=os.listdir(trshine)[i]\n",
        "  img=cv2.imread(trshine+f)\n",
        "  img=cv2.resize(img,size)\n",
        "  img=img[:,:,::-1]/255\n",
        "  tr_data[x]=img\n",
        "  tr_label.append(2)\n",
        "  x+=1\n",
        "for i in range(len(os.listdir(trsunrise))):\n",
        "  f=os.listdir(trsunrise)[i]\n",
        "  img=cv2.imread(trsunrise+f)\n",
        "  img=cv2.resize(img,size)\n",
        "  img=img[:,:,::-1]/255\n",
        "  tr_data[x]=img\n",
        "  tr_label.append(3)\n",
        "  x+=1\n",
        "\n",
        "tr_label=np.array(tr_label)"
      ],
      "metadata": {
        "id": "nlnlKZrxe1CB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tr_label.shape)"
      ],
      "metadata": {
        "id": "1FkVoF9JL_cA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert the test data to numpy\n",
        "te_filename=list()\n",
        "te_data=np.empty(shape=(75,100,100,3))\n",
        "for i in range(len(os.listdir(testpath))):\n",
        "  f=os.listdir(testpath)[i]\n",
        "  te_filename.append(f)\n",
        "  img=cv2.imread(testpath+f)\n",
        "  img=cv2.resize(img,(100,100))\n",
        "  img=img[:,:,::-1]/255\n",
        "  te_data[i]=img"
      ],
      "metadata": {
        "id": "7q2ZWbFcXEhQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# show data size\n",
        "print('train data size:',tr_data.shape)\n",
        "print('train label size:',tr_label.shape)\n",
        "print('test data size:',te_data.shape)"
      ],
      "metadata": {
        "id": "5rpm0LFMdkSu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### shuffle"
      ],
      "metadata": {
        "id": "jNYqCvrkn5UB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# on-hot label\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "tr_label=to_categorical(tr_label)\n",
        "# 打亂資料順序\n",
        "from sklearn.utils import shuffle\n",
        "tr_data,tr_label=shuffle(tr_data,tr_label,random_state=0)"
      ],
      "metadata": {
        "id": "htMr6KcXn2mq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### train_validation data split"
      ],
      "metadata": {
        "id": "EwYA8f8e7r9V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train_validation data split\n",
        "#val_data數量固定262個\n",
        "train_data=tr_data[0:1836]\n",
        "val_data=tr_data[1836:]\n",
        "train_label=tr_label[0:1836]\n",
        "val_label=tr_label[1836:]\n",
        "print('train data size: ',train_data.shape)\n",
        "print('validation data size: ',val_data.shape)\n",
        "print('train label size: ',train_label.shape)\n",
        "print('validation label size: ',val_label.shape)"
      ],
      "metadata": {
        "id": "gIfwyuSY5Ytc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: Build a simple CNN"
      ],
      "metadata": {
        "id": "7NCukwEIptQ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define model\n",
        "cnn=models.Sequential() # name the network\n",
        "# feature extraction\n",
        "\"\"\"cnn.add(layers.Conv2D(filters=32,kernel_size=(3,3), input_shape=(300,300,3),activation='relu',padding='same'))\n",
        "cnn.add(layers.Conv2D(filters=32,kernel_size=(3,3),activation='relu',padding='same'))\n",
        "cnn.add(layers.MaxPooling2D(pool_size=(2,2),padding='same'))\n",
        "cnn.add(layers.Conv2D(filters=64,kernel_size=(3,3),activation='relu',padding='same'))\n",
        "cnn.add(layers.Conv2D(filters=64,kernel_size=(3,3),activation='relu',padding='same'))\n",
        "cnn.add(layers.MaxPooling2D(pool_size=(2,2),padding='same'))\n",
        "cnn.add(layers.Flatten())\n",
        "# neron network\n",
        "cnn.add(layers.Dense(units=64,activation='relu'))\n",
        "cnn.add(layers.Dense(units=32,activation='relu'))\n",
        "cnn.add(layers.Dense(units=4,activation='softmax'))\"\"\"\n",
        "\n",
        "# feature extraction\n",
        "cnn.add(layers.Conv2D(filters=32, kernel_size=(3,3), input_shape=(100,100,3), activation='relu', padding='same'))\n",
        "cnn.add(layers.Conv2D(filters=32, kernel_size=(3,3), activation='relu', padding='same'))\n",
        "cnn.add(layers.MaxPooling2D(pool_size=(2,2), padding='same'))\n",
        "cnn.add(layers.Dropout(0.25))  # Add dropout here\n",
        "\n",
        "cnn.add(layers.Conv2D(filters=64, kernel_size=(3,3), activation='relu', padding='same'))\n",
        "cnn.add(layers.Conv2D(filters=64, kernel_size=(3,3), activation='relu', padding='same'))\n",
        "cnn.add(layers.MaxPooling2D(pool_size=(2,2), padding='same'))\n",
        "cnn.add(layers.Dropout(0.25))  # Add dropout here\n",
        "\n",
        "cnn.add(layers.Flatten())\n",
        "\n",
        "# neural network\n",
        "cnn.add(layers.Dense(units=64, activation='relu'))\n",
        "cnn.add(layers.Dropout(0.5))  # Add dropout here\n",
        "cnn.add(layers.Dense(units=32, activation='relu'))\n",
        "cnn.add(layers.Dropout(0.5))  # Add dropout here\n",
        "cnn.add(layers.Dense(units=4, activation='softmax'))\n",
        "\n",
        "# show the model structure\n",
        "cnn.summary()"
      ],
      "metadata": {
        "id": "whaKqelmUyAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# comiple model\n",
        "cnn.compile(optimizer='adam',loss=tf.keras.losses.CategoricalCrossentropy(),metrics=['accuracy'])\n",
        "\n",
        "class_weights = {0: 1.40703518,\n",
        "                1: 1.,\n",
        "                2: 1.16582915,\n",
        "                3: 1.69346734}"
      ],
      "metadata": {
        "id": "XjHVy0LXtnu6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "datagen = ImageDataGenerator(\n",
        "        rotation_range=20,\n",
        "        zoom_range=0.15,\n",
        "        width_shift_range=0.2,\n",
        "        height_shift_range=0.2,\n",
        "        shear_range=0.15,\n",
        "        horizontal_flip=True,\n",
        "        fill_mode=\"nearest\")"
      ],
      "metadata": {
        "id": "gw1yGr3mGMhm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils import class_weight\n",
        "#history = cnn.fit(train_data, train_label,batch_size=10 ,epochs=100,validation_split=0.3)\n",
        "history = cnn.fit_generator(datagen.flow(train_data, train_label, batch_size=32), validation_data=(val_data, val_label), epochs=100, class_weight=class_weights)"
      ],
      "metadata": {
        "id": "NswWOJ8ivbr7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['loss'],label='loss')\n",
        "plt.plot(history.history['val_loss'],label='val_loss')\n",
        "plt.title('loss curve')\n",
        "plt.ylabel('loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "plt.plot(history.history['accuracy'],label='accuracy')\n",
        "plt.plot(history.history['val_accuracy'],label='val_accuracy')\n",
        "plt.title('accuracy curve')\n",
        "plt.ylabel('accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "adbKficbHBw-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot the confusion matrix"
      ],
      "metadata": {
        "id": "9rJG7OpAqoeG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "pre=cnn.predict(val_data)\n",
        "pre=np.argmax(pre,axis=1)"
      ],
      "metadata": {
        "id": "lKks-N98qi4k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sn\n",
        "cm=confusion_matrix(pre,np.argmax(val_label,axis=1))\n",
        "fit=plt.figure(figsize=(8,6))\n",
        "plt.title('confusion matrix')\n",
        "sn.heatmap(cm,annot=True,cmap='OrRd',fmt='g')\n",
        "plt.xlabel('prediction')\n",
        "plt.ylabel('true label')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "X9L9D56jfWgV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Output the csv file"
      ],
      "metadata": {
        "id": "iyxz9yabGz-9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "lQgtDt_X-woc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction=cnn.predict(te_data)\n",
        "prediction=np.argmax(prediction,axis=1)\n",
        "prediction"
      ],
      "metadata": {
        "id": "KGQCs62dHADk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_label=pd.DataFrame()\n",
        "test_label['image_id']=te_filename\n",
        "test_label['labels']=prediction\n",
        "test_label=test_label.sort_values(by='image_id')\n",
        "test_label.to_csv('/content/drive/MyDrive/weather_image/predict_label.csv',index=False) #結果轉csv檔"
      ],
      "metadata": {
        "id": "-rtuKWmSHti9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part3: Data augmentation"
      ],
      "metadata": {
        "id": "lt3l2o5LijGa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_augmentation = tf.keras.Sequential([\n",
        "  layers.RandomFlip(\"horizontal_and_vertical\"),\n",
        "  layers.RandomRotation(0.5),\n",
        "])"
      ],
      "metadata": {
        "id": "PFXmZEdgiTvr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data=tr_data[1]\n",
        "data=data_augmentation(data)\n",
        "plt.imshow(data)"
      ],
      "metadata": {
        "id": "N49mNujO5FIb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}